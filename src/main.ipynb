{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, StableDiffusionPipeline, DDIMScheduler\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "import gc\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# model_id = \"stabilityai/stable-diffusion-2-1\"\n",
    "model_id = \"stabilityai/stable-diffusion-2-base\"\n",
    "torch_device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=4): # to be reproducible\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(images, model_name):\n",
    "    if not os.path.exists(f\"../experiments/{model_name}/images\"):\n",
    "        os.makedirs(f\"../experiments/{model_name}/images\")  \n",
    "    for i in range(len(images)):\n",
    "        images[i].save(f\"../experiments/{model_name}/images/img_{i}.jpg\")\n",
    "\n",
    "def save_models(model_name,unet=None,text_encoder=None):\n",
    "    if unet is not None:\n",
    "        unet.save_pretrained(f\"../experiments/{model_name}/unet\")\n",
    "    if text_encoder is not None:\n",
    "        text_encoder.save_pretrained(f\"../experiments/{model_name}/text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stuff\n",
    "vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_id, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    model_id, subfolder=\"text_encoder\",\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    model_id, subfolder=\"unet\"\n",
    ")\n",
    "scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "\n",
    "#keep all models on GPU (unless specified otherwise)\n",
    "text_encoder.to(torch_device)\n",
    "vae = vae.to(torch_device)\n",
    "unet = unet.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overloading the same data set for dreambooth images and class images \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir, resolution=512, pre_load_images=True, class_of_obj=\"\"):\n",
    "        self.data_dir = data_dir\n",
    "        self.resolution = resolution\n",
    "        self.images=[]\n",
    "        self.pre_load_images = pre_load_images\n",
    "        self.class_of_obj = class_of_obj\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(self.resolution),\n",
    "                transforms.RandomCrop(self.resolution),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "        if not os.path.exists(data_dir):\n",
    "            raise Exception(f\"this dataset doesnt exist: {data_dir}\")\n",
    "        self.images_names = os.listdir(data_dir)\n",
    "        print(f\"{self.__len__()} images in {data_dir}\")\n",
    "        if self.pre_load_images: #if its the dreambooth dataset, keep on ram (~4 images in total)\n",
    "            self.__loadimages__()\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_names)\n",
    "\n",
    "    def __loadimages__(self):\n",
    "        self.images = []\n",
    "        for idx in range(len(self.images_names)):\n",
    "            img_path = os.path.join(self.data_dir, self.images_names[idx])\n",
    "            image = Image.open(img_path)  \n",
    "            image = ImageOps.exif_transpose(image) # PIL kept rotating images randomly?\n",
    "\n",
    "            self.images.append(image)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.pre_load_images: # used for dreambooth dataset (which should be preloaded)\n",
    "            return self.transform(self.images[idx])\n",
    "        else: # used for pregenerated class dataset \n",
    "            img_path = os.path.join(self.data_dir, f\"{self.class_of_obj}_{idx%(self.__len__())}.jpeg\")\n",
    "            \n",
    "            image = Image.open(img_path)  \n",
    "            image = ImageOps.exif_transpose(image)\n",
    "            return self.transform(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 images in ./db/misha_rembg_db\n",
      "500 images in ./class_pictures/class_dog/v2-base_512\n"
     ]
    }
   ],
   "source": [
    "model_name = \"misha_lora_best\" # name to use for saving the model after training\n",
    "\n",
    "# ------------------------------------------args here--------------------------------------\n",
    "batchsize = 1                           # dont touch, since it doesnt fit on GPU.............\n",
    "use_prior = [False, True][1]            # prior loss, needs the class img datasets to compare performance\n",
    "prior_scaler = 1.                       # for prior loss\n",
    "weave_losses = [False, True][1]         # to make everything fit on memory, do 2 backprops\n",
    "                                        # (only when using prior loss)\n",
    "gradient_checkpointing = [False, True][0] # siiiiigh, if only i knew this exists sooner.... kinda makes weave_losses poitnless\n",
    "train_text = [\"False\",\"lora\",\"True\"][1] # how to train text encoder\n",
    "unet_use_lora = [False, True][1]\n",
    "rank = 200                              # lora rank (for both text and unet)\n",
    "adam8bit = [False,True][0]              # use 8bit adam \n",
    "total_train_steps = 500                 # totol number of optim steps\n",
    "grad_accumilation = 1                   # accumulate gradients every x steps\n",
    "lr =  3e-5                              # lora uses bigger learning rates\n",
    "\n",
    "save_period=0                           # save every x epochs, if 0 dont save\n",
    "\n",
    "resolution = 512                        # 512 or 768 for v2-1 (if feeling genorous)\n",
    "class_of_obj = [\"woman\",\"dog\", \"\"][1]\n",
    "\n",
    "if not use_prior:\n",
    "    weave_losses = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt = [f\"a photo of sks {class_of_obj}\"]\n",
    "text_tokens = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "text_inputs = text_tokens.input_ids.to(torch_device)\n",
    "if train_text == \"False\":\n",
    "    with torch.no_grad():\n",
    "        text_embeddings = text_encoder(text_inputs)[0]\n",
    "        print(\"precomputed the text embedding\")\n",
    "\n",
    "# ------------------------------------------dreambooth dataset (3~5 images)----------\n",
    "my_dbs = [\"dog_db\",\"dorsa_rembg_db\",\"hyeny_rembg_db\",\"misha_db\",\"misha_rembg_db\"][-1]\n",
    "ds = MyDataset(data_dir=f\"./db/{my_dbs}\", \n",
    "               pre_load_images=True,\n",
    "               resolution=resolution)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=batchsize,\n",
    "        shuffle=True,\n",
    "        num_workers=1,  # <--------------------------remove this if using windows----------------\n",
    "    )\n",
    "\n",
    "if use_prior:\n",
    "    class_prompt = [f\"a photo of {class_of_obj}\"]\n",
    "    class_text_tokens = tokenizer(\n",
    "        class_prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "    class_text_inputs = class_text_tokens.input_ids.to(torch_device)\n",
    "\n",
    "    if train_text == \"False\":\n",
    "        with torch.no_grad():\n",
    "            class_text_embeddings = text_encoder(class_text_inputs)[0]\n",
    "            print(\"precomputed the class text embedding\")\n",
    "\n",
    "    model_versions = {\"stabilityai/stable-diffusion-2-base\":\"v2-base\",\n",
    "                    \"stabilityai/stable-diffusion-2-1\":\"v2-1\"}\n",
    "    # proper class dataset should already exist\n",
    "    class_dir = f\"{model_versions[model_id]}_{resolution}\"\n",
    "    path_to_class_images = f\"./class_pictures/class_{class_of_obj}/{class_dir}\"\n",
    "\n",
    "    class_ds = MyDataset(data_dir=path_to_class_images,\n",
    "                        pre_load_images=False,\n",
    "                        resolution=resolution, \n",
    "                        class_of_obj=class_of_obj)\n",
    "    class_dataloader = torch.utils.data.DataLoader(\n",
    "            class_ds,\n",
    "            batch_size=batchsize,\n",
    "            shuffle=True,\n",
    "            num_workers=1,  # <--------------------------remove this if using windows----------------\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using lora for unet\n",
      "using lora for text encoder\n",
      "gradient chekpointing enabled\n"
     ]
    }
   ],
   "source": [
    "vae.requires_grad_(False)\n",
    "\n",
    "if unet_use_lora:\n",
    "    unet = unet.requires_grad_(False)\n",
    "    print(\"using lora for unet\")\n",
    "    unet_lora_config = LoraConfig(\n",
    "            r=rank,\n",
    "            lora_alpha=rank,\n",
    "            init_lora_weights=\"gaussian\",\n",
    "            target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\", \"add_k_proj\", \"add_v_proj\"],\n",
    "        )\n",
    "    unet = get_peft_model(unet, unet_lora_config)\n",
    "else:\n",
    "    print(\"full unet training\")\n",
    "    unet = unet.requires_grad_(True)\n",
    "unet.train()\n",
    "\n",
    "\n",
    "if train_text == \"lora\":\n",
    "    print(\"using lora for text encoder\")\n",
    "    text_encoder = text_encoder.requires_grad_(False)\n",
    "    text_lora_config = LoraConfig(\n",
    "            r=rank,\n",
    "            lora_alpha=rank,\n",
    "            init_lora_weights=\"gaussian\",\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "        )\n",
    "    text_encoder = get_peft_model(text_encoder, text_lora_config)\n",
    "    text_encoder.train()\n",
    "elif train_text == \"True\":\n",
    "    print(\"full text training\")\n",
    "    text_encoder = text_encoder.requires_grad_(True)\n",
    "    text_encoder.train()\n",
    "elif train_text == \"False\":\n",
    "    print(\"text encoder is frozen\")\n",
    "    text_encoder = text_encoder.requires_grad_(False)\n",
    "    text_encoder = text_encoder.to(\"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    raise Exception(\"wtf is text encoder doing?\")\n",
    "\n",
    "if gradient_checkpointing:\n",
    "    print(\"gradient chekpointing enabled\")\n",
    "    unet.enable_gradient_checkpointing()\n",
    "    if train_text != \"False\":\n",
    "        text_encoder.gradient_checkpointing_enable()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using adam\n",
      "total of 79180800 trainable floats over 440 tensors\n"
     ]
    }
   ],
   "source": [
    "trainable_params = []\n",
    "for p in unet.parameters():\n",
    "    if p.requires_grad:\n",
    "        trainable_params.append(p)\n",
    "\n",
    "for p in text_encoder.parameters():\n",
    "    if p.requires_grad:\n",
    "        trainable_params.append(p)\n",
    "\n",
    "if adam8bit: \n",
    "    optim = AdamW8bit(trainable_params, lr=lr)\n",
    "    print(\"using 8bit adam\")\n",
    "else:\n",
    "    optim = AdamW(trainable_params, lr=lr)\n",
    "    print(\"using adam\")\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, total_train_steps, eta_min=lr/2)\n",
    "\n",
    "numel = 0\n",
    "for tensor in trainable_params:\n",
    "    numel += tensor.numel()\n",
    "print(f\"total of {numel} trainable floats over {len(trainable_params)} tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feed_forward(batch, timesteps, embeddings):\n",
    "    with torch.no_grad():\n",
    "        model_input = vae.encode(batch).latent_dist.sample()  # hmmm can i get the latent_dist only once and keep in memory and sample from it?\n",
    "        model_input = model_input * vae.config.scaling_factor # tried it, dont know how to handle the object.......need to try more\n",
    "\n",
    "    noise = torch.randn_like(model_input, device=torch_device)\n",
    "\n",
    "    noisy_images = scheduler.add_noise(model_input, noise, timesteps)\n",
    "\n",
    "    if scheduler.config.prediction_type == \"v_prediction\":\n",
    "        target = scheduler.get_velocity(model_input, noise, timesteps)\n",
    "    else: \n",
    "        target = noise\n",
    "\n",
    "    noise_pred = unet(noisy_images, timesteps, embeddings)[0]\n",
    "\n",
    "    loss = F.mse_loss(noise_pred, target)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=0 \n",
    "# if you get tired of training, just interupt jupyter and test the model :)\n",
    "# why save checkpoints when you can just....not.....\n",
    "# then resume training like nothing has happened  :)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using prior class loss\n",
      "unet using lora\n",
      "text training: lora\n",
      "using adamw\n",
      "weaving losses\n",
      "500 steps over 83 epochs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6306e335251844b794db3192bec7247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parsa/miniconda3/envs/dbooth_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "unet.train()\n",
    "if train_text != \"False\":\n",
    "    text_encoder.train()\n",
    "\n",
    "\n",
    "grad_acc_counter = 0\n",
    "total_epochs = int(total_train_steps/(len(dataloader)/grad_accumilation))\n",
    "print(f'{\"using prior class loss\" if use_prior else \"\"}')\n",
    "print(f'{\"unet using lora\" if unet_use_lora else \"full unet training\"}')\n",
    "print(f'text training: {train_text}')\n",
    "print(f'using {\"8bit \" if adam8bit else \"\"}adamw')\n",
    "print(f'{\"weaving losses\" if weave_losses else \"\"}')\n",
    "print(f'{total_train_steps} steps over {total_epochs} epochs')\n",
    "\n",
    "\n",
    "pbar = tqdm(total=total_epochs)\n",
    "while epoch<total_epochs:\n",
    "    if save_period!=0 and epoch%save_period==0 and epoch!=0:\n",
    "        save_models(f\"{model_name}_{epoch}\",unet=unet, text_encoder=text_encoder)\n",
    "    epoch+=1\n",
    "    for step,batch in enumerate(dataloader):\n",
    "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batchsize,), device=torch_device, dtype=torch.int64)\n",
    "\n",
    "        batch = batch.to(torch_device)\n",
    "\n",
    "        if train_text != \"False\":\n",
    "            text_embeddings = text_encoder(text_inputs)[0]\n",
    "        loss = train_feed_forward(batch, timesteps, text_embeddings) / grad_accumilation\n",
    "\n",
    "\n",
    "        if weave_losses:\n",
    "            loss.backward()\n",
    "\n",
    "        if use_prior:\n",
    "\n",
    "            class_batch = next(iter(class_dataloader))\n",
    "            class_batch = class_batch.to(torch_device)\n",
    "\n",
    "            if train_text != \"False\":\n",
    "                class_text_embeddings = text_encoder(class_text_inputs)[0]\n",
    "            prior_loss = train_feed_forward(class_batch, timesteps, class_text_embeddings) / grad_accumilation\n",
    "            prior_loss = prior_scaler * prior_loss \n",
    "            \n",
    "            if weave_losses:\n",
    "                prior_loss.backward()\n",
    "\n",
    "            loss = loss + prior_loss\n",
    "\n",
    "        if not weave_losses:\n",
    "            loss.backward()\n",
    "        # raise Exception()\n",
    "\n",
    "        grad_acc_counter+=1\n",
    "        if grad_acc_counter==grad_accumilation:\n",
    "            grad_acc_counter=0\n",
    "            torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)\n",
    "            torch.cuda.empty_cache()\n",
    "            optim.step()\n",
    "            lr_scheduler.step()\n",
    "            optim.zero_grad()  \n",
    "\n",
    "    pbar.set_description(f\"epoch: {epoch}/{total_epochs} lr: {lr_scheduler.get_last_lr()[0]}\")\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.zero_grad()\n",
    "batch = None\n",
    "noise = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fa5a7528e54b51a03aa4c31ae6fc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected types for text_encoder: (<class 'transformers.models.clip.modeling_clip.CLIPTextModel'>,), got <class 'peft.peft_model.PeftModel'>.\n",
      "Expected types for unet: (<class 'diffusers.models.unets.unet_2d_condition.UNet2DConditionModel'>,), got <class 'peft.peft_model.PeftModel'>.\n"
     ]
    }
   ],
   "source": [
    "unet.eval()\n",
    "text_encoder.eval()\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id,\n",
    "                                               vae=vae,\n",
    "                                               text_encoder=text_encoder,\n",
    "                                               unet=unet,\n",
    "                                               scheduler=scheduler,\n",
    "                                               )\n",
    "pipe = pipe.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts for woman class\n",
    "\n",
    "# prompts = [\"sks woman in a field of roses\",\n",
    "#            \"sks woman walking a dog\",\n",
    "#            \"sks woman fighting a demon\",\n",
    "#            \"sks woman with a very expensive purse\",\n",
    "#            \"sks woman wearing expensive shoes\",\n",
    "#            \"sks woman cleaning her room\"\n",
    "# ]\n",
    "# images = []\n",
    "# for pr in prompts:\n",
    "#     with torch.no_grad():\n",
    "#         images.append(pipe(pr).images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce3033afc6f45c598dfbd8714724a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3be53c962b45b4bf74b85965fde6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2e25f83706404e95dec4b6fb574127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06bb3cdefde4890b8d000ee0014ba5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee8be913ca64ce7a7f13b763de97b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prompts for dog class\n",
    "\n",
    "prompts = [f\"sks {class_of_obj} in a field of roses\",\n",
    "           f\"sks {class_of_obj} pooping\",\n",
    "           f\"sks {class_of_obj} catching a frisbee while in air\",\n",
    "           f\"sks {class_of_obj} playing with another dog\",\n",
    "           f\"sks {class_of_obj} being an angel and flying over a city\"\n",
    "]\n",
    "images = []\n",
    "for pr in prompts:\n",
    "    set_seed(1)\n",
    "    with torch.no_grad():\n",
    "        images.append(pipe(pr).images[0])\n",
    "\n",
    "if save_period!=0:\n",
    "    save_models(model_name, unet=unet, text_encoder=text_encoder)   \n",
    "\n",
    "# save_images(images, model_name)\n",
    "# print(f\"saved {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# to see the result. \u001b[39;00m\n\u001b[32m      3\u001b[39m i=\u001b[32m5\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mprompts\u001b[49m[i])\n\u001b[32m      5\u001b[39m images[i]\n",
      "\u001b[31mNameError\u001b[39m: name 'prompts' is not defined"
     ]
    }
   ],
   "source": [
    "# to see the result. \n",
    "\n",
    "i=5\n",
    "print(prompts[i])\n",
    "images[i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbooth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
