batchsize = 1                           # dont touch, since it doesnt fit on GPU.............
use_prior = [False, True][1]            # prior loss, needs the class img datasets to compare performance
prior_scaler = 1.
weave_losses = [False, True][0]         # to increase memory efficency when using prior loss.
                                        # applies prior loss and regular loss one at a time
train_text = ["False","lora","True"][2]
unet_use_lora = [False, True][0]
rank = 50                              # lora rank
adam8bit = [False,True][1]              # use 8bit adam
total_train_steps = 250
grad_accumilation = 1                   # accumulate gradients every x steps
lr =  1e-4 if unet_use_lora else 5e-6   #lora uses bigger learning rates
# lr = 5e-6
resolution = 512
class_of_obj = ["woman","dog"][1] 



using prior class loss
full unet training
text training: True
using 8bit adamw

250 steps over 41 epochs
epoch: 41/41 mean loss: 0.15040727433127662 lr: 2.5015788042424785e-06: 100%|██████████| 41/41 [08:58<00:00, 13.13s/it]
Timer unit: 1e-07 s

Total time: 538.257 s
File: C:\Users\merat\AppData\Local\Temp\ipykernel_18928\618756271.py
Function: to_profile at line 5

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     5                                           def to_profile():
     6                                           
     7         1          4.0      4.0      0.0      grad_acc_counter = 0
     8         1        257.0    257.0      0.0      total_epochs = int(total_train_steps/(len(dataloader)/grad_accumilation))
     9         1        931.0    931.0      0.0      print(f'{"using prior class loss" if use_prior else ""}')
    10         1        116.0    116.0      0.0      print(f'{"unet using lora" if unet_use_lora else "full unet training"}')
    11         1        102.0    102.0      0.0      print(f'text training: {train_text}')
    12         1        105.0    105.0      0.0      print(f'using {"8bit " if adam8bit else ""}adamw')
    13         1        109.0    109.0      0.0      print(f'{"weaving losses" if weave_losses else ""}')
    14         1        114.0    114.0      0.0      print(f'{total_train_steps} steps over {total_epochs} epochs')
    15                                           
    16         1          3.0      3.0      0.0      epoch = 0
    17         1      36410.0  36410.0      0.0      pbar = tqdm(total=total_epochs)
    18        42        294.0      7.0      0.0      while epoch<total_epochs:
    19        41        144.0      3.5      0.0          epoch+=1
    20        41        472.0     11.5      0.0          logs = {"epoch_loss":0., "epoch_prior_loss":0., "epoch_db_loss":0.}
    21       287   80351702.0 279971.1      1.5          for step,batch in enumerate(dataloader):
    22       246    9142530.0  37164.8      0.2              timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (batchsize,), device=torch_device, dtype=torch.int64)
    23                                           
    24       246     940011.0   3821.2      0.0              batch = batch.to(torch_device)
    25                                           
    26       246       3443.0     14.0      0.0              if train_text != "False":
    27       246   36593445.0 148753.8      0.7                  text_embeddings = text_encoder(text_inputs)[0]
    28       246  816993477.0    3e+06     15.2              loss = train_feed_forward(batch, timesteps, text_embeddings) / grad_accumilation
    29                                           
    30       246       5512.0     22.4      0.0              if do_log:
    31       246  810062324.0    3e+06     15.0                  logs["epoch_db_loss"] += loss.item()
    32       246       6134.0     24.9      0.0              if weave_losses:
    33                                                           loss.backward()
    34                                           
    35       246       1882.0      7.7      0.0              if use_prior:
    36       246   14563827.0  59202.5      0.3                  class_batch = next(iter(class_dataloader))
    37       246     904522.0   3676.9      0.0                  class_batch = class_batch.to(torch_device)
    38                                           
    39       246       2571.0     10.5      0.0                  if train_text != "False":
    40       246   29446774.0 119702.3      0.5                      class_text_embeddings = text_encoder(class_text_inputs)[0]
    41       246  810237251.0    3e+06     15.1                  prior_loss = train_feed_forward(class_batch, timesteps, class_text_embeddings) / grad_accumilation
    42       246     183459.0    745.8      0.0                  prior_loss = prior_scaler * prior_loss 
    43                                                           
    44       246       3648.0     14.8      0.0                  if do_log:
    45       246  810360575.0    3e+06     15.1                      logs["epoch_prior_loss"] += prior_loss.item()
    46       246       6256.0     25.4      0.0                  if weave_losses:
    47                                                               prior_loss.backward()
    48                                           
    49       246     272278.0   1106.8      0.0                  loss = loss + prior_loss
    50                                           
    51       246       2408.0      9.8      0.0              if not weave_losses:
    52       246 1382775886.0    6e+06     25.7                  loss.backward()
    53       246   53074537.0 215750.2      1.0              logs["epoch_loss"] += loss.item()
    54                                           
    55       246       3378.0     13.7      0.0              grad_acc_counter+=1
    56       246       2976.0     12.1      0.0              if grad_acc_counter==grad_accumilation:
    57       246       1111.0      4.5      0.0                  grad_acc_counter=0
    58       246   15446265.0  62789.7      0.3                  torch.nn.utils.clip_grad_norm_(trainable_params, 1.0)
    59       246  505252423.0    2e+06      9.4                  optim.step()
    60       246      64769.0    263.3      0.0                  lr_scheduler.step()
    61       246    5127007.0  20841.5      0.1                  optim.zero_grad()
    62                                                   
    63       164       1115.0      6.8      0.0          for k,v in logs.items():
    64       123       9232.0     75.1      0.0              logs[k] = v*grad_accumilation/len(dataloader)
    65                                           
    66        41        319.0      7.8      0.0          if do_log:
    67        41      37912.0    924.7      0.0              writer.add_scalar("loss/total_loss", logs["epoch_loss"],  epoch) 
    68        41        166.0      4.0      0.0              if use_prior:
    69        41      10163.0    247.9      0.0                  writer.add_scalar("loss/db_loss", logs["epoch_db_loss"],  epoch) 
    70        41       7969.0    194.4      0.0                  writer.add_scalar("loss/prior_loss", logs["epoch_prior_loss"],  epoch) 
    71        41     351767.0   8579.7      0.0          pbar.set_description(f"epoch: {epoch}/{total_epochs} mean loss: {logs['epoch_loss']} lr: {lr_scheduler.get_last_lr()[0]}")
    72        41     264445.0   6449.9      0.0          pbar.update(1)
    73                                           
    74                                               
    75         1      14296.0  14296.0      0.0      writer.close()